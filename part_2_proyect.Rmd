---
title: "Part-2"
author: 'Florencia Luque  and Seyed Amirhossein Mosaddad '
output: pdf_document
date: "2024-10-18"
---
## Part 2


```{r message=FALSE, warning=FALSE}
library(caret)
library(caretEnsemble)
library(h2o)
library(tidymodels)
library(ROCR)
library(ConfusionTableR)
library(dplyr)
library(pander)
```
## Introduction

This dataset is a data obtain from *kaggle* and is used to predict if a pacient will probably get a stroke based on characteristic of them like gender, age, bmi, glucose levels.\

The stroke variable have a 4.8% of people have had one. We want to check the distributions of the variables and possibles explanations of which variable can make an impact to get a stroke before creating a model to proved or been proved wrong about it.\

The data have the follow variables:\

1)  id: unique identifier
2)  gender: "Male", "Female" or "Other"
3)  age: age of the patient
4)  hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension
5)  heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease
6)  ever_married: "No" or "Yes"
7)  work_type: "children", "Govt_jov", "Never_worked", "Private" or "Self-employed"
8)  Residence_type: "Rural" or "Urban"
9)  avg_glucose_level: average glucose level in blood
10) bmi: body mass index
11) smoking_status: "formerly smoked", "never smoked", "smokes" or "Unknown"
12) stroke: 1 if the patient had a stroke or 0 if not


```{r warning=FALSE}
path = "/Users/soroush/Desktop/UC3M/courses/R/minitask1/Stroke_data"
stroke_data = read.csv("healthcare-dataset-stroke-data.csv",header = TRUE)

stroke_data = na.omit(stroke_data)
stroke_data = stroke_data[stroke_data$gender!="Other",]
stroke_data$gender = as.factor(stroke_data$gender)
stroke_data$hypertension = as.factor(stroke_data$hypertension)
stroke_data$heart_disease = as.factor(stroke_data$heart_disease)
stroke_data$work_type = as.factor(stroke_data$work_type)
stroke_data$Residence_type = as.factor(stroke_data$Residence_type)
stroke_data$ever_married = as.factor(stroke_data$ever_married)
stroke_data$smoking_status = as.factor(stroke_data$smoking_status)
stroke_data$stroke = as.factor(stroke_data$stroke)
stroke_data$bmi =as.numeric(stroke_data$bmi)

frec_table <- cut(stroke_data$bmi, breaks = c(0, 18.5, 24.9, 29.9, 34.9, Inf), 
                labels = c("Underweight", "Normal", "Overweight", "Obese", "Extremely Obese"))
stroke_data$cat_weight = frec_table

glucose_categories <- cut(stroke_data$avg_glucose_level, 
                          breaks = c(0, 99.9, 125.9, Inf),
                          labels = c("Normal", "Prediabetes", "Diabetes"))

# Reorder the levels to put Normal in the middle
glucose_categories <- factor(glucose_categories, levels = c("Prediabetes", "Normal", "Diabetes"))

# Add the categories to the data frame
stroke_data$glucose_category = glucose_categories
str(stroke_data)
```


### Relation between variables

#### Stroke with Gender

```{r,warning=FALSE}
tab_st_gd = table(stroke_data$stroke,stroke_data$gender)
chisq.test(tab_st_gd)
```
The *p-value* is larger than 0.05 this mean that there's not evidence of dependency between the variables gender and stroke. 

#### Stroke with Hypertension

```{r,warning=FALSE}
tab_st_hy = table(stroke_data$stroke,stroke_data$hypertension)
chisq.test(tab_st_hy)
```
The *p-value* is a lot smaller than 0.05. This mean that there's a relation between getting a stroke and hypertension. This is a could be a comprobation of the hypothesis that we established earlier about the existence of a relation between this two variables.

#### Stroke with Heart Disease

```{r,warning=FALSE}
tab_st_hd = table(stroke_data$stroke,stroke_data$heart_disease)
chisq.test(tab_st_hd)
```
The *p-value* is a lot smaller than 0.05. This mean that there's a relation between getting a stroke and heart disease This is a could be a comprobation of the hypothesis that we established earlier about the existence of a relation between this two variables.


#### Stroke with Residence Type

```{r,warning=FALSE}
tab_st_rt = table(stroke_data$stroke,stroke_data$Residence_type)
chisq.test(tab_st_rt)
```
As we had seen in the graph there´s no evidence to say that there's a relation between the type of residence and getting a stroke.

#### Stroke with Ever Married

```{r,warning=FALSE}
tab_st_em = table(stroke_data$stroke,stroke_data$ever_married)
chisq.test(tab_st_em)
```
Apparently there's a relation within this two variables. Having a stroke have a relation with have been or had been ever married. 

#### Stroke with Smoking Status

```{r,warning=FALSE}
tab_st_sk = table(stroke_data$stroke,stroke_data$smoking_status)
chisq.test(tab_st_sk)
```
The *p-value* is a lot smaller than 0.05 so there's is a relation between the variables. This was something that we *dont know what to write here*


#### Stroke with Work Type

```{r,warning=FALSE}
tab_st_wt = table(stroke_data$stroke,stroke_data$work_type)
chisq.test(tab_st_wt)
```
There's a relation between the variables (*p-value*<0.05). This we think was because of the difference between the quantity of people who got a stroke and work independently and the people who work with children because the difference was big between them.

#### Heart Disease and Hypertension

```{r,warning=FALSE}
tab_hy_hd = table(stroke_data$heart_disease,stroke_data$hypertension)
chisq.test(tab_hy_hd)
```
There's a relation between hypertension and heart disease and both variable are related to stroke. This could be a good indicator that within only one of this variables we could have the same information in the model.

#### Heart Disease and Ever Married

```{r,warning=FALSE}
tab_hd_em = table(stroke_data$heart_disease,stroke_data$ever_married)
chisq.test(tab_hd_em)
```
#### Heart Disease and Smoking Status

```{r,warning=FALSE}
tab_hd_sk= table(stroke_data$heart_disease,stroke_data$smoking_status)
chisq.test(tab_hd_sk)
```

#### Heart Disease and Work Type

```{r,warning=FALSE}
tab_hd_wt = table(stroke_data$heart_disease,stroke_data$work_type)
chisq.test(tab_hd_wt)
```
#### Hypertension and Ever Married

```{r,warning=FALSE}
tab_hy_em = table(stroke_data$heart_disease,stroke_data$ever_married)
chisq.test(tab_hy_em)
```

#### Hypertension and Smoking Status

```{r,warning=FALSE}
tab_hy_sk = table(stroke_data$heart_disease,stroke_data$smoking_status)
chisq.test(tab_hy_sk)
```

#### Hypertension and Work Type

```{r,warning=FALSE}
tab_hy_wt= table(stroke_data$heart_disease,stroke_data$work_type)
chisq.test(tab_hy_wt)
```

### Stroke with Age

```{r warning=FALSE}
t.test(age ~ stroke, data = stroke_data)

```
As you can see we reject the null hypothesis that said that both group have the same mean and this tell as that this variable could have and impact in the probability of getting a stroke in this case getting older increase your chances.

### Stroke with Average Glucose Levels


```{r warning=FALSE}
t.test(avg_glucose_level ~ stroke, data = stroke_data)
```
Also we can say that the difference in means between the groups with a stroke are without is not zero. 

### Stroke with BMI
```{r warning=FALSE}
t.test(bmi ~ stroke, data = stroke_data)
```
Apparently all the continuous variables could have and impact in the chances or getting a stroke. This variable *bmi* also have a *p-value* less than 0.05 so we can say that the groups have significant different means. 

## Models

### Logistic Regresion (Flo)
```{r warning=F}
set.seed(23)
index_split = createDataPartition(stroke_data$stroke,p=0.8,list=FALSE)
train = stroke_data[index_split,]
test = stroke_data[-index_split,]
train = subset(train,select = -c(id,cat_weight,glucose_category))
train = na.omit(train)
test = subset(test,select = -c(id,cat_weight,glucose_category))
test = na.omit(test)
```

We would start the models with one that includes all the variables and the reduce it with different test.

```{r,warning=FALSE}
class_weight <- ifelse(train$stroke == 1, 25, 1.04)
log_reg_model = glm(stroke~(gender + age + hypertension + heart_disease + ever_married +
    work_type + Residence_type + avg_glucose_level + bmi + smoking_status),data = train,family = binomial(link='logit'),weights = class_weight)
summary(log_reg_model)
```


```{r warning=FALSE}

fitted.results <- predict(log_reg_model,newdata = test,type='response')
fitted.results <- ifelse(fitted.results > 0.8,1,0)
test$accu = fitted.results
misClasificError <- mean(fitted.results != test$stroke)
print(paste('Accuracy',1-misClasificError))
pander(confusionMatrix(as.factor(test$accu),test$stroke))

```
```{r warning=FALSE}
#pander(confusionMatrix(test$accu,test$stroke))
```
As you can see if we take 0.8 as the threshold we get an accuracy of 0.89 and a sensibility of 0.9.
We will continue deleting the variable resident type because it's not hace any significance in the model.

```{r,warning=FALSE}
class_weight <- ifelse(train$stroke == 1, 25, 1.04)
log_reg_model2 = glm(stroke~(gender + age + hypertension + heart_disease + ever_married +work_type + avg_glucose_level + bmi + smoking_status),data = train,family = binomial(link='logit'),weights = class_weight)
summary(log_reg_model2)

fitted.results <- predict(log_reg_model2,newdata = test,type='response')
fitted.results <- ifelse(fitted.results > 0.8,1,0)
test$accu = fitted.results
misClasificError <- mean(fitted.results != test$stroke)
print(paste('Accuracy',1-misClasificError))

pander(confusionMatrix(as.factor(test$accu),test$stroke))
```
The AIC was reduce so the model improved just a little.

```{r warning=FALSE}
anova(log_reg_model2,log_reg_model)
```
As the *p-value* is higher than 0.05 we cannot reject that the simpler model is better. So we are going to deleted the variable gender because i doesn't affect the model.

```{r,warning=FALSE}
class_weight <- ifelse(train$stroke == 1, 25, 1.04)
log_reg_model3 = glm(stroke~(ever_married + age + hypertension + heart_disease+work_type + avg_glucose_level + bmi + smoking_status),data = train,family = binomial(link='logit'),weights = class_weight)
summary(log_reg_model3)

fitted.results <- predict(log_reg_model3,newdata = test,type='response')
fitted.results <- ifelse(fitted.results > 0.8,1,0)
test$accu = fitted.results
misClasificError <- mean(fitted.results != test$stroke)
print(paste('Accuracy',1-misClasificError))

pander(confusionMatrix(as.factor(test$accu),test$stroke))
```
```{r warning=FALSE}
anova(log_reg_model3,log_reg_model2)
```
As the *p-value* is higher than 0.05 we cannot reject that the simpler model is better. So we are going to deleted the variable ever married because i doesn't affect the model.


```{r,warning=FALSE}
class_weight <- ifelse(train$stroke == 1, 25, 1.04)
log_reg_model4 = glm(stroke~(age + hypertension + heart_disease+work_type + avg_glucose_level + bmi + smoking_status),data = train,family = binomial(link='logit'),weights = class_weight)
summary(log_reg_model4)

fitted.results <- predict(log_reg_model4,newdata = test,type='response')
fitted.results <- ifelse(fitted.results > 0.8,1,0)
test$accu = fitted.results
misClasificError <- mean(fitted.results != test$stroke)
print(paste('Accuracy',1-misClasificError))

pander(confusionMatrix(as.factor(test$accu),test$stroke))
```
```{r warning=FALSE}
anova(log_reg_model,log_reg_model4)
```
The simpler model is better so as all the variables are significance in the model we will leave it at that. 

```{r warning=FALSE}

pander(exp(log_reg_model4$coefficients))

```
With the exp of the coefficients we get the odds of getting a stroke. Like the intercept means that if all the other variables are zero you have a 2.1% odds of getting a stroke. As we said in the preliminary analysis if you got hypertension the odd of getting a stroke duplicate. If you got a heart disease you have a 4.5% increase in your odds. 

### VSM (soroush)



### XGBoost for classification (soroush)


### H2O models with autoh2o (Flo)





Split the data to get a 80% for training and 20% for testing.#flo

```{r warning=FALSE}
h2o.init()
stroke_h2o=as.h2o(stroke_data)
split_data = h2o.splitFrame(data=stroke_h2o,ratios=0.8,seed=23)
train_h2o = split_data[[1]]
test_h2o = split_data[[2]]
predictor = c("gender","age","hypertension","heart_disease","ever_married","work_type","Residence_type","avg_glucose_level","bmi","smoking_status")
aml = h2o.automl(x = predictor,y="stroke",training_frame=train_h2o,max_models=10,keep_cross_validation_predictions = TRUE,nfolds = 10,stopping_metric = "AUC",balance_classes = TRUE,seed = 23)
lb <- aml@leaderboard
print(lb, n = nrow(lb),extra_columns="ALL")

```
We are going to take as a initial point to create models the best 3 algorithm. In this case we will be comparing the Grading bosting machine, Generalized linear models and the eXtremely Randomized Trees.

#### Gradieng Bosting Machine
```{r warning=FALSE}
gbm_model = h2o.gbm(x = predictor,y = "stroke",training_frame = train_h2o,keep_cross_validation_predictions = TRUE,nfolds = 10,seed = 23)

```

The performance of this model is the follow.

```{r}
gbm_perform = h2o.performance(gbm_model,newdata = test_h2o)
print(gbm_perform)

```

#### Generalized Linear Models


```{r warning=FALSE}
glm_model = h2o.glm(x = predictor, y = "stroke", training_frame = train_h2o,keep_cross_validation_predictions = TRUE,nfolds = 10,family = "binomial",seed = 23)

```

The performance of this model is the follow.

```{r}
glm_perform = h2o.performance(glm_model,newdata = test_h2o)
print(glm_perform)
```

#### Extremely Randomized Trees

This model is a type of random Forest who takes as many trees as predictors you have.

```{r warning=FALSE}
xrt_model = h2o.randomForest(x = predictor, y = "stroke",training_frame = train_h2o,keep_cross_validation_predictions = TRUE,nfolds = 10,mtries = length(predictor),seed = 23)

```
The performance of this model is the follow.

```{r}
xrt_perform = h2o.performance(xrt_model,newdata = test_h2o)
print(xrt_perform)
```
When comparing the three models, we observe that each achieves high accuracy, above 80% on the test data. However, considering the variable we aim to predict—the probability of having a stroke—we want to minimize the number of high-risk individuals predicted as low-risk. This means reducing false negatives. The model that best achieves this is the GLM, which still maintains a high accuracy of 0.874.

#### Ensemble model
We will create an ensemble model with the 3 models that we just created.

```{r warning=FALSE}
base_models=list(gbm_model@model_id,xrt_model@model_id,glm_model@model_id)

ensemble_model=h2o.stackedEnsemble(x=predictor,y="stroke", training_frame=train_h2o, base_models=base_models)
```
Now we can check the perfomance of the new model.
```{r warning=FALSE}
perf_ensemble=h2o.performance(ensemble_model,newdata=test_h2o)
print(perf_ensemble)

```
The ensemble model increased accuracy by 1%, reaching a value of 88%. False positives were also reduced, so overall, the ensemble model improved performance and is better suited for the data

